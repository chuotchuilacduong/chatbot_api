import os
import torch
import uvicorn
from contextlib import asynccontextmanager
from typing import Dict, AsyncGenerator

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# --- LangChain Imports (ƒê√£ c·∫≠p nh·∫≠t chu·∫©n m·ªõi) ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_classic.chains import create_retrieval_chain, create_history_aware_retriever
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory 
from langchain_community.chat_message_histories import ChatMessageHistory 
from langchain_core.runnables.history import RunnableWithMessageHistory

# --- C·∫•u h√¨nh ---
class Settings:
    LLM_MODEL: str = "qwen2.5:7b"
    EMBEDDING_MODEL_ID: str = "bkai-foundation-models/vietnamese-bi-encoder"
    DATA_PATH: str = "./du_lieu_cong_ty.txt"
    DB_DIR: str = "./vector_db_bkai"
    DEVICE: str = 'cuda' if torch.cuda.is_available() else 'cpu'

settings = Settings()

# B·ªô nh·ªõ chat (In-memory)
store: Dict[str, BaseChatMessageHistory] = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

class ChatRequest(BaseModel):
    query: str  
    session_id: str = "default_session"

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("üöÄ ƒêang kh·ªüi ƒë·ªông Server AI...")
    print(f"‚öôÔ∏è  Thi·∫øt b·ªã: {settings.DEVICE.upper()}")

    # 1. Setup Embedding
    embedding_model = HuggingFaceEmbeddings(
        model_name=settings.EMBEDDING_MODEL_ID,
        model_kwargs={'device': settings.DEVICE}, 
        encode_kwargs={'normalize_embeddings': True}
    )

    # 2. Setup Vector DB
    # Ki·ªÉm tra xem DB ƒë√£ t·ªìn t·∫°i v√† c√≥ d·ªØ li·ªáu kh√¥ng
    if os.path.exists(settings.DB_DIR) and os.listdir(settings.DB_DIR):
        print("üìÇ ƒêang t·∫£i Vector DB t·ª´ ·ªï c·ª©ng...")
        vector_db = Chroma(persist_directory=settings.DB_DIR, embedding_function=embedding_model)
    else:
        print("üî® ƒêang t·∫°o m·ªõi Vector DB...")
        if not os.path.exists(settings.DATA_PATH):
            # T·∫°o file m·∫´u n·∫øu ch∆∞a c√≥ ƒë·ªÉ tr√°nh crash
            with open(settings.DATA_PATH, "w", encoding="utf-8") as f:
                f.write("D·ªØ li·ªáu m·∫´u c√¥ng ty VietCivil ID Solutions.")
            print(f"‚ö†Ô∏è ƒê√£ t·∫°o file m·∫´u t·∫°i {settings.DATA_PATH}")
            
        loader = TextLoader(settings.DATA_PATH, encoding="utf-8")
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100) # TƒÉng chunk size m·ªôt ch√∫t
        chunks = text_splitter.split_documents(docs)
        vector_db = Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory=settings.DB_DIR)

    llm = OllamaLLM(model=settings.LLM_MODEL, temperature=0.1)
    
    retriever = vector_db.as_retriever(search_kwargs={"k": 4}) # TƒÉng k l√™n 4

    contextualize_q_system_prompt = (
        "D·ª±a tr√™n l·ªãch s·ª≠ tr√≤ chuy·ªán v√† c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng, "
        "h√£y vi·∫øt l·∫°i th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m√† kh√¥ng c·∫ßn ng·ªØ c·∫£nh c≈©. "
        "KH√îNG tr·∫£ l·ªùi, ch·ªâ vi·∫øt l·∫°i c√¢u h·ªèi ho·∫∑c gi·ªØ nguy√™n n·∫øu ƒë√£ r√µ r√†ng."
    )
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    qa_system_prompt = """
        B·∫°n l√† Tr·ª£ l√Ω AI chuy√™n tr√°ch h·ªó tr·ª£ ng∆∞·ªùi d√πng s·ª≠ d·ª•ng "H·ªá th·ªëng Qu·∫£n l√Ω D√¢n c∆∞". Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn ng·ªØ c·∫£nh (Context) d∆∞·ªõi ƒë√¢y.

        Quy t·∫Øc tr·∫£ l·ªùi:
        1. **Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong Context:** Kh√¥ng ƒë∆∞·ª£c t·ª± b·ªãa ra th√¥ng tin, quy tr√¨nh ho·∫∑c t√≠nh nƒÉng kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p. N·∫øu th√¥ng tin kh√¥ng c√≥ trong Context, h√£y tr·∫£ l·ªùi: "Xin l·ªói, hi·ªán t·∫°i t√†i li·ªáu h∆∞·ªõng d·∫´n ch∆∞a c·∫≠p nh·∫≠t th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
        2. **Vai tr√≤ v√† Ph√¢n quy·ªÅn:** Lu√¥n l∆∞u √Ω ƒë·∫øn vai tr√≤ c·ªßa ng∆∞·ªùi d√πng (T·ªï tr∆∞·ªüng, T·ªï ph√≥, C√°n b·ªô) n·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn quy·ªÅn h·∫°n (v√≠ d·ª•: xem b√°o c√°o, th·ªëng k√™).
        3. **Phong c√°ch tr√¨nh b√†y:**
        - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, chuy√™n nghi·ªáp, gi·ªçng vƒÉn h√†nh ch√≠nh nh∆∞ng th√¢n thi·ªán.
        - N·∫øu l√† quy tr√¨nh c√°c b∆∞·ªõc, h√£y s·ª≠ d·ª•ng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ƒë√°nh s·ªë (1, 2, 3...) ƒë·ªÉ d·ªÖ theo d√µi.
        - C√°c t√™n n√∫t b·∫•m, t√™n menu, ho·∫∑c tr·∫°ng th√°i (v√≠ d·ª•: "M·ªõi sinh", "ƒê√£ qua ƒë·ªùi") n√™n ƒë∆∞·ª£c ƒë·∫∑t trong d·∫•u ngo·∫∑c k√©p ho·∫∑c in ƒë·∫≠m ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ nh·∫≠n bi·∫øt.
        4. **X·ª≠ l√Ω t√¨nh hu·ªëng:**
        - N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ "nh·∫≠p li·ªáu cho tr·∫ª s∆° sinh", h√£y nh·∫Øc h·ªç kh√¥ng c·∫ßn ƒëi·ªÅn ngh·ªÅ nghi·ªáp/CCCD.
        - N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ "nhi·ªÅu ng∆∞·ªùi c√πng ph·∫£n √°nh", h√£y h∆∞·ªõng d·∫´n t√≠nh nƒÉng "G·ªôp ki·∫øn ngh·ªã".

        Ng·ªØ c·∫£nh (Context):
        {context}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    # L∆∞u chain v√†o app.state thay v√¨ bi·∫øn global
    app.state.final_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    )
    
    print("‚úÖ Server ƒë√£ s·∫µn s√†ng ph·ª•c v·ª•!")
    yield
    print("üõë Server ƒëang t·∫Øt...")

app = FastAPI(title="VietCivil ID Chatbot API", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],  
    allow_headers=["*"],  
)

@app.post("/chat_stream")
async def chat_stream_endpoint(request: ChatRequest, req: Request):
    # Ki·ªÉm tra chain t·ª´ app.state
    if not hasattr(req.app.state, "final_chain"):
        raise HTTPException(status_code=503, detail="H·ªá th·ªëng AI ch∆∞a s·∫µn s√†ng ho·∫∑c ƒëang kh·ªüi ƒë·ªông.")

    final_chain = req.app.state.final_chain

    async def generate_response() -> AsyncGenerator[str, None]:
        config = {"configurable": {"session_id": request.session_id}}
        
        try:
            async for chunk in final_chain.astream(
                {"input": request.query}, 
                config=config
            ):
                if "answer" in chunk:
                    # Tr·∫£ v·ªÅ t·ª´ng token/chunk
                    yield chunk["answer"]
        except Exception as e:
            yield f"\n[L·ªói h·ªá th·ªëng: {str(e)}]"
    
    return StreamingResponse(generate_response(), media_type="text/plain")

if __name__ == "__main__":
    uvicorn.run("api_server:app", host="0.0.0.0", port=8000, reload=True)